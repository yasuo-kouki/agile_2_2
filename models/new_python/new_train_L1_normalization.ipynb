{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ppm (P3) を読み，RGB値を 0.00-1.00 に正規化して\n",
    "Python のリスト形式で出力 & 可能ならクリップボードへコピーするスクリプト\n",
    "\"\"\"\n",
    "\n",
    "def read_p3_ppm(path):\n",
    "    tokens = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # コメント(#)以降を取り除き，残りのトークンを追加\n",
    "            line = line.split('#', 1)[0].strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            tokens.extend(line.split())\n",
    "\n",
    "    if len(tokens) < 4:\n",
    "        raise ValueError(\"ppmファイルの形式が正しくありません\")\n",
    "\n",
    "    if tokens[0] != 'P3':\n",
    "        raise ValueError(\"P3形式のppmを指定してください\")\n",
    "\n",
    "    # ヘッダを読み取る\n",
    "    idx = 1\n",
    "    # 幅と高さ\n",
    "    width = int(tokens[idx]); idx += 1\n",
    "    height = int(tokens[idx]); idx += 1\n",
    "    maxval = int(tokens[idx]); idx += 1\n",
    "\n",
    "    expected_values = width * height * 3\n",
    "    remaining = tokens[idx:]\n",
    "    if len(remaining) < expected_values:\n",
    "        raise ValueError(f\"ピクセルデータが不足しています（期待 {expected_values} 個，見つかった {len(remaining)} 個）\")\n",
    "\n",
    "    # ピクセルを読み取り，0-1に正規化（小数点2桁に丸め）\n",
    "    pixels = []\n",
    "    for i in range(0, expected_values, 3):\n",
    "        r = int(remaining[i])\n",
    "        g = int(remaining[i+1])\n",
    "        b = int(remaining[i+2])\n",
    "        pixels.append([round(r / maxval, 2), round(g / maxval, 2), round(b / maxval, 2)])\n",
    "\n",
    "    return pixels, width, height\n",
    "\n",
    "def format_as_python_list(pixels, width, height):\n",
    "    \"\"\" 見やすく各行ごとに出力するリスト表記を作る \"\"\"\n",
    "    lines = []\n",
    "    lines.append('[')\n",
    "    for row in range(height):\n",
    "        row_pixels = pixels[row*width:(row+1)*width]\n",
    "        # 各画素を [x.xx, y.yy, z.zz] の形で並べる\n",
    "        pixel_strs = [f\"[{p[0]:.2f}, {p[1]:.2f}, {p[2]:.2f}]\" for p in row_pixels]\n",
    "        lines.append('    ' + ', '.join(pixel_strs) + (',' if row != height-1 else ''))\n",
    "    lines.append(']')\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def copy_to_clipboard(text):\n",
    "    \"\"\" pyperclip があればそれを使い，なければ tkinter を試す \"\"\"\n",
    "    try:\n",
    "        import pyperclip\n",
    "        pyperclip.copy(text)\n",
    "        return True\n",
    "    except Exception:\n",
    "        try:\n",
    "            # tkinter を使う方法（環境によってはXサーバ/GUIが必要）\n",
    "            import tkinter as tk\n",
    "            r = tk.Tk()\n",
    "            r.withdraw()\n",
    "            r.clipboard_clear()\n",
    "            r.clipboard_append(text)\n",
    "            r.update()  # クリップボードに渡す\n",
    "            r.destroy()\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    path = input(\"読み込む ppm（P3）ファイル名を入力してください：\").strip()\n",
    "    try:\n",
    "        pixels, w, h = read_p3_ppm(path)\n",
    "    except Exception as e:\n",
    "        print(\"エラー：\", e)\n",
    "        return\n",
    "\n",
    "    out_text = format_as_python_list(pixels, w, h)\n",
    "    print(\"\\n出力（コピー可能な形式）．\\n\")\n",
    "    print(out_text)\n",
    "\n",
    "    if copy_to_clipboard(out_text):\n",
    "        print(\"\\nクリップボードにコピーしました．\")\n",
    "    else:\n",
    "        print(\"\\nクリップボードにコピーできませんでした．必要なら出力を選択してコピーしてください．\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 推論データ\n",
    "X =  np.array([\n",
    "    [0.63, 0.12, 0.07], [0.24, 0.88, 0.22], [0.07, 0.28, 0.40],\n",
    "    [0.88, 0.99, 0.28], [0.40, 0.98, 0.91], [0.56, 0.20, 0.27],\n",
    "    [0.00, 0.00, 0.00], [0.28, 0.53, 0.36], [0.97, 1.00, 0.99],\n",
    "\n",
    "    [0.22, 0.13, 0.06], [0.65, 0.64, 0.36], [0.15, 0.43, 0.42], [0.15, 0.37, 0.10], [0.34, 0.55, 0.55], [0.28, 0.96, 0.68],\n",
    "    [0.68, 0.38, 0.08], [0.02, 0.25, 0.37], [0.60, 0.14, 0.11], [0.11, 0.05, 0.10], [0.49, 0.97, 0.22], [0.76, 0.74, 0.16],\n",
    "    [0.01, 0.08, 0.18], [0.08, 0.59, 0.16], [0.49, 0.09, 0.03], [0.83, 0.97, 0.23], [0.59, 0.16, 0.19], [0.09, 0.57, 0.57],\n",
    "    [1.00, 1.00, 1.00], [0.68, 0.97, 0.74], [0.44, 0.71, 0.49], [0.25, 0.45, 0.30], [0.11, 0.17, 0.13], [0.00, 0.03, 0.01],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "img1 = cv2.resize(cv2.imread(\"img/reference_image1_cmyk_large.png\", cv2.IMREAD_COLOR_RGB), (3,3), interpolation=cv2.INTER_NEAREST)\n",
    "img2 = cv2.resize(cv2.imread(\"img/reference_image2_cmyk_large.png\", cv2.IMREAD_COLOR_RGB), (6,4), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "plt.imshow(img1)\n",
    "plt.show()\n",
    "plt.imshow(img2)\n",
    "plt.show()\n",
    "\n",
    "# 正解データ\n",
    "Y = np.concatenate([img1.reshape([-1, 3])/255, img2.reshape([-1, 3])/255], axis=0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50000]  MSE: 0.445629  L1: 292.13  Loss: 0.445921\n",
      "Epoch [500/50000]  MSE: 0.000908  L1: 224.78  Loss: 0.001133\n",
      "Epoch [1000/50000]  MSE: 0.000228  L1: 221.57  Loss: 0.000450\n",
      "Epoch [1500/50000]  MSE: 0.000101  L1: 206.70  Loss: 0.000308\n",
      "Epoch [2000/50000]  MSE: 0.000205  L1: 196.10  Loss: 0.000401\n",
      "Epoch [2500/50000]  MSE: 0.000168  L1: 183.65  Loss: 0.000351\n",
      "Epoch [3000/50000]  MSE: 0.000044  L1: 170.92  Loss: 0.000215\n",
      "Epoch [3500/50000]  MSE: 0.000013  L1: 158.59  Loss: 0.000171\n",
      "Epoch [4000/50000]  MSE: 0.000054  L1: 148.35  Loss: 0.000202\n",
      "Epoch [4500/50000]  MSE: 0.000025  L1: 140.91  Loss: 0.000166\n",
      "Epoch [5000/50000]  MSE: 0.000008  L1: 132.55  Loss: 0.000141\n",
      "Epoch [5500/50000]  MSE: 0.000013  L1: 127.50  Loss: 0.000141\n",
      "Epoch [6000/50000]  MSE: 0.000029  L1: 121.78  Loss: 0.000150\n",
      "Epoch [6500/50000]  MSE: 0.000287  L1: 117.49  Loss: 0.000404\n",
      "Epoch [7000/50000]  MSE: 0.000009  L1: 113.55  Loss: 0.000122\n",
      "Epoch [7500/50000]  MSE: 0.000004  L1: 109.55  Loss: 0.000113\n",
      "Epoch [8000/50000]  MSE: 0.000002  L1: 106.60  Loss: 0.000109\n",
      "Epoch [8500/50000]  MSE: 0.000003  L1: 103.60  Loss: 0.000107\n",
      "Epoch [9000/50000]  MSE: 0.000009  L1: 101.06  Loss: 0.000110\n",
      "Epoch [9500/50000]  MSE: 0.000002  L1: 99.17  Loss: 0.000101\n",
      "Epoch [10000/50000]  MSE: 0.000137  L1: 99.45  Loss: 0.000237\n",
      "Epoch [10500/50000]  MSE: 0.000041  L1: 95.23  Loss: 0.000137\n",
      "Epoch [11000/50000]  MSE: 0.000004  L1: 93.40  Loss: 0.000098\n",
      "Epoch [11500/50000]  MSE: 0.000009  L1: 91.65  Loss: 0.000100\n",
      "Epoch [12000/50000]  MSE: 0.000010  L1: 89.67  Loss: 0.000099\n",
      "Epoch [12500/50000]  MSE: 0.000014  L1: 87.85  Loss: 0.000102\n",
      "Epoch [13000/50000]  MSE: 0.000004  L1: 86.04  Loss: 0.000090\n",
      "Epoch [13500/50000]  MSE: 0.000002  L1: 85.28  Loss: 0.000088\n",
      "Epoch [14000/50000]  MSE: 0.000003  L1: 83.41  Loss: 0.000086\n",
      "Epoch [14500/50000]  MSE: 0.000007  L1: 83.04  Loss: 0.000090\n",
      "Epoch [15000/50000]  MSE: 0.000002  L1: 81.34  Loss: 0.000083\n",
      "Epoch [15500/50000]  MSE: 0.000008  L1: 80.71  Loss: 0.000089\n",
      "Epoch [16000/50000]  MSE: 0.000002  L1: 79.62  Loss: 0.000082\n",
      "Epoch [16500/50000]  MSE: 0.000049  L1: 79.15  Loss: 0.000128\n",
      "Epoch [17000/50000]  MSE: 0.000009  L1: 78.14  Loss: 0.000087\n",
      "Epoch [17500/50000]  MSE: 0.000001  L1: 77.95  Loss: 0.000079\n",
      "Epoch [18000/50000]  MSE: 0.000001  L1: 77.09  Loss: 0.000078\n",
      "Epoch [18500/50000]  MSE: 0.000008  L1: 77.00  Loss: 0.000085\n",
      "Epoch [19000/50000]  MSE: 0.000011  L1: 76.10  Loss: 0.000087\n",
      "Epoch [19500/50000]  MSE: 0.000008  L1: 75.61  Loss: 0.000083\n",
      "Epoch [20000/50000]  MSE: 0.000002  L1: 74.95  Loss: 0.000077\n",
      "Epoch [20500/50000]  MSE: 0.000007  L1: 74.30  Loss: 0.000081\n",
      "Epoch [21000/50000]  MSE: 0.000002  L1: 73.95  Loss: 0.000076\n",
      "Epoch [21500/50000]  MSE: 0.000002  L1: 73.86  Loss: 0.000076\n",
      "Epoch [22000/50000]  MSE: 0.000009  L1: 73.15  Loss: 0.000082\n",
      "Epoch [22500/50000]  MSE: 0.000012  L1: 73.19  Loss: 0.000085\n",
      "Epoch [23000/50000]  MSE: 0.000008  L1: 72.64  Loss: 0.000081\n",
      "Epoch [23500/50000]  MSE: 0.000010  L1: 72.88  Loss: 0.000083\n",
      "Epoch [24000/50000]  MSE: 0.000038  L1: 72.34  Loss: 0.000110\n",
      "Epoch [24500/50000]  MSE: 0.000158  L1: 72.06  Loss: 0.000230\n",
      "Epoch [25000/50000]  MSE: 0.000002  L1: 71.72  Loss: 0.000074\n",
      "Epoch [25500/50000]  MSE: 0.000014  L1: 71.31  Loss: 0.000086\n",
      "Epoch [26000/50000]  MSE: 0.000042  L1: 70.76  Loss: 0.000113\n",
      "Epoch [26500/50000]  MSE: 0.000001  L1: 70.63  Loss: 0.000072\n",
      "Epoch [27000/50000]  MSE: 0.000009  L1: 70.40  Loss: 0.000080\n",
      "Epoch [27500/50000]  MSE: 0.000041  L1: 70.43  Loss: 0.000111\n",
      "Epoch [28000/50000]  MSE: 0.000002  L1: 70.03  Loss: 0.000072\n",
      "Epoch [28500/50000]  MSE: 0.000013  L1: 70.02  Loss: 0.000083\n",
      "Epoch [29000/50000]  MSE: 0.000002  L1: 69.65  Loss: 0.000072\n",
      "Epoch [29500/50000]  MSE: 0.000030  L1: 70.06  Loss: 0.000100\n",
      "Epoch [30000/50000]  MSE: 0.000003  L1: 69.52  Loss: 0.000072\n",
      "Epoch [30500/50000]  MSE: 0.000003  L1: 69.56  Loss: 0.000073\n",
      "Epoch [31000/50000]  MSE: 0.000007  L1: 69.50  Loss: 0.000077\n",
      "Epoch [31500/50000]  MSE: 0.000007  L1: 69.85  Loss: 0.000076\n",
      "Epoch [32000/50000]  MSE: 0.000005  L1: 69.41  Loss: 0.000074\n",
      "Epoch [32500/50000]  MSE: 0.000001  L1: 68.98  Loss: 0.000070\n",
      "Epoch [33000/50000]  MSE: 0.000018  L1: 68.85  Loss: 0.000087\n",
      "Epoch [33500/50000]  MSE: 0.000001  L1: 68.58  Loss: 0.000070\n",
      "Epoch [34000/50000]  MSE: 0.000045  L1: 68.63  Loss: 0.000114\n",
      "Epoch [34500/50000]  MSE: 0.000002  L1: 68.65  Loss: 0.000071\n",
      "Epoch [35000/50000]  MSE: 0.000003  L1: 68.58  Loss: 0.000071\n",
      "Epoch [35500/50000]  MSE: 0.000031  L1: 68.89  Loss: 0.000100\n",
      "Epoch [36000/50000]  MSE: 0.000001  L1: 68.26  Loss: 0.000070\n",
      "Epoch [36500/50000]  MSE: 0.000017  L1: 68.60  Loss: 0.000086\n",
      "Epoch [37000/50000]  MSE: 0.000001  L1: 68.24  Loss: 0.000069\n",
      "Epoch [37500/50000]  MSE: 0.000011  L1: 68.21  Loss: 0.000079\n",
      "Epoch [38000/50000]  MSE: 0.000003  L1: 68.01  Loss: 0.000071\n",
      "Epoch [38500/50000]  MSE: 0.000009  L1: 67.95  Loss: 0.000077\n",
      "Epoch [39000/50000]  MSE: 0.000017  L1: 68.43  Loss: 0.000085\n",
      "Epoch [39500/50000]  MSE: 0.000012  L1: 67.87  Loss: 0.000080\n",
      "Epoch [40000/50000]  MSE: 0.000003  L1: 67.92  Loss: 0.000071\n",
      "Epoch [40500/50000]  MSE: 0.000002  L1: 67.92  Loss: 0.000070\n",
      "Epoch [41000/50000]  MSE: 0.000027  L1: 68.62  Loss: 0.000096\n",
      "Epoch [41500/50000]  MSE: 0.000001  L1: 67.91  Loss: 0.000069\n",
      "Epoch [42000/50000]  MSE: 0.000002  L1: 68.20  Loss: 0.000070\n",
      "Epoch [42500/50000]  MSE: 0.000002  L1: 67.45  Loss: 0.000070\n",
      "Epoch [43000/50000]  MSE: 0.000001  L1: 67.83  Loss: 0.000069\n",
      "Epoch [43500/50000]  MSE: 0.000005  L1: 67.60  Loss: 0.000072\n",
      "Epoch [44000/50000]  MSE: 0.000009  L1: 67.97  Loss: 0.000077\n",
      "Epoch [44500/50000]  MSE: 0.000002  L1: 67.80  Loss: 0.000070\n",
      "Epoch [45000/50000]  MSE: 0.000002  L1: 67.64  Loss: 0.000069\n",
      "Epoch [45500/50000]  MSE: 0.000022  L1: 67.52  Loss: 0.000089\n",
      "Epoch [46000/50000]  MSE: 0.000004  L1: 68.11  Loss: 0.000073\n",
      "Epoch [46500/50000]  MSE: 0.000001  L1: 67.13  Loss: 0.000068\n",
      "Epoch [47000/50000]  MSE: 0.000001  L1: 67.07  Loss: 0.000068\n",
      "Epoch [47500/50000]  MSE: 0.000016  L1: 68.52  Loss: 0.000084\n",
      "Epoch [48000/50000]  MSE: 0.000001  L1: 67.48  Loss: 0.000068\n",
      "Epoch [48500/50000]  MSE: 0.000014  L1: 66.70  Loss: 0.000081\n",
      "Epoch [49000/50000]  MSE: 0.000002  L1: 66.76  Loss: 0.000069\n",
      "Epoch [49500/50000]  MSE: 0.000012  L1: 66.84  Loss: 0.000079\n",
      "\n",
      "予測結果：\n",
      "[[0.92515844 0.12047973 0.14519638]\n",
      " [0.4090624  0.7387001  0.27044   ]\n",
      " [0.21982457 0.32742843 0.64374053]\n",
      " [0.957926   0.9186983  0.078642  ]\n",
      " [0.4330945  0.79635185 0.8686552 ]\n",
      " [0.7172646  0.32026178 0.62192327]\n",
      " [0.00135047 0.00856078 0.00892471]\n",
      " [0.49766612 0.49938777 0.5005155 ]\n",
      " [0.9930437  0.99533856 1.0012194 ]\n",
      " [0.45185387 0.30970007 0.25942713]\n",
      " [0.7617839  0.5607625  0.49704456]\n",
      " [0.35582995 0.47061807 0.6126054 ]\n",
      " [0.35519242 0.422178   0.2527982 ]\n",
      " [0.5150075  0.4965348  0.6868956 ]\n",
      " [0.373765   0.73979706 0.6800728 ]\n",
      " [0.867527   0.4771557  0.19088534]\n",
      " [0.269143   0.35640267 0.65599334]\n",
      " [0.77180296 0.31364253 0.38002846]\n",
      " [0.3614353  0.23208785 0.4139679 ]\n",
      " [0.61437243 0.73506767 0.23186645]\n",
      " [0.8898452  0.6288566  0.15945067]\n",
      " [0.15310809 0.24697633 0.56764835]\n",
      " [0.23379259 0.57962805 0.27503562]\n",
      " [0.69814765 0.21949786 0.23199403]\n",
      " [0.91422707 0.77570593 0.11026756]\n",
      " [0.7538028  0.30798864 0.57409006]\n",
      " [0.0040331  0.5196614  0.647947  ]\n",
      " [0.93590564 0.93968564 0.9151313 ]\n",
      " [0.78352875 0.7913775  0.7906326 ]\n",
      " [0.6277821  0.6330212  0.63479924]\n",
      " [0.4750957  0.47524458 0.4800894 ]\n",
      " [0.32868713 0.33329505 0.339705  ]\n",
      " [0.19066823 0.20044242 0.20294157]]\n",
      "\n",
      "C++ 用のパラメータファイル (model_parameters.h) を作成しました。\n"
     ]
    }
   ],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "# モデル定義\n",
    "class ColorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = ColorNet()\n",
    "\n",
    "# 損失関数（MSE）\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# L1正則化の係数 (重みを0を多くなるように定義している)\n",
    "lambda_l1 = 1e-6\n",
    "\n",
    "# 学習ループ\n",
    "epochs = 50000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_tensor)\n",
    "    mse = criterion(outputs, Y_tensor)\n",
    "\n",
    "    # --- L1 正則化-----------------------------\n",
    "    l1 = torch.tensor(0.0, requires_grad=False)\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            l1 = l1 + p.abs().sum()\n",
    "    loss = mse + lambda_l1 * l1\n",
    "    # -----------------------------------------\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch [{epoch}/{epochs}]  MSE: {mse.item():.6f}  L1: {l1.item():.2f}  Loss: {loss.item():.6f}')\n",
    "\n",
    "threshold = 0.01 # しきい値の設定\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_tensor)\n",
    "    print(\"\\n予測結果：\")\n",
    "    print(predictions.numpy())\n",
    "\n",
    "    \n",
    "# C++ 用の配列形式に変換する関数\n",
    "def convert_to_cpp_array(tensor: torch.Tensor, name: str, dtype: str = \"float\",threshold:float=0.01):\n",
    "    # Tensor → numpy 配列\n",
    "    flat = tensor.detach().numpy().flatten()\n",
    "    \n",
    "    # しきい値以下の値を0にする\n",
    "    # where(条件, 条件がTrueのときの値, 条件がFalseのときの値)\n",
    "    flat = np.where(np.abs(flat) < threshold, 0, flat)\n",
    "    \n",
    "    array_str = f\"{dtype} {name}[] = {{\"\n",
    "    array_str += \", \".join(map(str, flat))\n",
    "    array_str += \"};\"\n",
    "    return array_str\n",
    "\n",
    "\n",
    "\n",
    "# C++ 用のパラメータファイルを作成\n",
    "cpp_code = \"\"\n",
    "\n",
    "layer_idx = 1\n",
    "for layer in model.model:\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        cpp_code += convert_to_cpp_array(layer.weight, f\"weight_{layer_idx}\") + \"\\n\"\n",
    "        cpp_code += convert_to_cpp_array(layer.bias,   f\"bias_{layer_idx}\") + \"\\n\"\n",
    "        layer_idx += 1\n",
    "\n",
    "with open(\"model_parameters.h\", \"w\") as f:\n",
    "    f.write(cpp_code)\n",
    "\n",
    "print(\"\\nC++ 用のパラメータファイル (model_parameters.h) を作成しました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50000]  MSE: 0.335274  L1: 775.85  Loss: 0.336050\n",
      "Epoch [500/50000]  MSE: 0.000225  L1: 529.02  Loss: 0.000755\n",
      "Epoch [1000/50000]  MSE: 0.000078  L1: 407.68  Loss: 0.000486\n",
      "Epoch [1500/50000]  MSE: 0.000066  L1: 324.46  Loss: 0.000390\n",
      "Epoch [2000/50000]  MSE: 0.000029  L1: 267.70  Loss: 0.000297\n",
      "Epoch [2500/50000]  MSE: 0.000007  L1: 231.92  Loss: 0.000239\n",
      "Epoch [3000/50000]  MSE: 0.000047  L1: 205.36  Loss: 0.000252\n",
      "Epoch [3500/50000]  MSE: 0.000007  L1: 185.16  Loss: 0.000192\n",
      "Epoch [4000/50000]  MSE: 0.000073  L1: 175.05  Loss: 0.000248\n",
      "Epoch [4500/50000]  MSE: 0.000044  L1: 169.39  Loss: 0.000213\n",
      "Epoch [5000/50000]  MSE: 0.000029  L1: 157.50  Loss: 0.000187\n",
      "Epoch [5500/50000]  MSE: 0.000023  L1: 146.83  Loss: 0.000170\n",
      "Epoch [6000/50000]  MSE: 0.000008  L1: 137.92  Loss: 0.000146\n",
      "Epoch [6500/50000]  MSE: 0.000011  L1: 130.85  Loss: 0.000141\n",
      "Epoch [7000/50000]  MSE: 0.000023  L1: 125.55  Loss: 0.000148\n",
      "Epoch [7500/50000]  MSE: 0.000045  L1: 122.23  Loss: 0.000167\n",
      "Epoch [8000/50000]  MSE: 0.000057  L1: 116.84  Loss: 0.000174\n",
      "Epoch [8500/50000]  MSE: 0.000063  L1: 115.12  Loss: 0.000178\n",
      "Epoch [9000/50000]  MSE: 0.000071  L1: 111.41  Loss: 0.000182\n",
      "Epoch [9500/50000]  MSE: 0.000003  L1: 108.89  Loss: 0.000112\n",
      "Epoch [10000/50000]  MSE: 0.000003  L1: 106.91  Loss: 0.000110\n",
      "Epoch [10500/50000]  MSE: 0.000024  L1: 105.22  Loss: 0.000129\n",
      "Epoch [11000/50000]  MSE: 0.000011  L1: 104.45  Loss: 0.000116\n",
      "Epoch [11500/50000]  MSE: 0.000007  L1: 102.57  Loss: 0.000109\n",
      "Epoch [12000/50000]  MSE: 0.000004  L1: 100.54  Loss: 0.000104\n",
      "Epoch [12500/50000]  MSE: 0.000002  L1: 99.34  Loss: 0.000102\n",
      "Epoch [13000/50000]  MSE: 0.000002  L1: 98.08  Loss: 0.000100\n",
      "Epoch [13500/50000]  MSE: 0.000007  L1: 96.77  Loss: 0.000104\n",
      "Epoch [14000/50000]  MSE: 0.000005  L1: 95.01  Loss: 0.000100\n",
      "Epoch [14500/50000]  MSE: 0.000020  L1: 94.30  Loss: 0.000115\n",
      "Epoch [15000/50000]  MSE: 0.000001  L1: 92.40  Loss: 0.000094\n",
      "Epoch [15500/50000]  MSE: 0.000003  L1: 91.57  Loss: 0.000095\n",
      "Epoch [16000/50000]  MSE: 0.000001  L1: 91.10  Loss: 0.000092\n",
      "Epoch [16500/50000]  MSE: 0.000003  L1: 90.23  Loss: 0.000093\n",
      "Epoch [17000/50000]  MSE: 0.000001  L1: 89.28  Loss: 0.000091\n",
      "Epoch [17500/50000]  MSE: 0.000051  L1: 89.88  Loss: 0.000141\n",
      "Epoch [18000/50000]  MSE: 0.000027  L1: 88.61  Loss: 0.000115\n",
      "Epoch [18500/50000]  MSE: 0.000003  L1: 87.56  Loss: 0.000091\n",
      "Epoch [19000/50000]  MSE: 0.000013  L1: 87.29  Loss: 0.000100\n",
      "Epoch [19500/50000]  MSE: 0.000002  L1: 86.56  Loss: 0.000089\n",
      "Epoch [20000/50000]  MSE: 0.000006  L1: 86.85  Loss: 0.000093\n",
      "Epoch [20500/50000]  MSE: 0.000011  L1: 86.03  Loss: 0.000097\n",
      "Epoch [21000/50000]  MSE: 0.000004  L1: 85.47  Loss: 0.000090\n",
      "Epoch [21500/50000]  MSE: 0.000142  L1: 85.74  Loss: 0.000227\n",
      "Epoch [22000/50000]  MSE: 0.000003  L1: 85.58  Loss: 0.000088\n",
      "Epoch [22500/50000]  MSE: 0.000005  L1: 85.26  Loss: 0.000090\n",
      "Epoch [23000/50000]  MSE: 0.000001  L1: 84.52  Loss: 0.000085\n",
      "Epoch [23500/50000]  MSE: 0.000001  L1: 84.65  Loss: 0.000085\n",
      "Epoch [24000/50000]  MSE: 0.000088  L1: 85.86  Loss: 0.000174\n",
      "Epoch [24500/50000]  MSE: 0.000002  L1: 84.04  Loss: 0.000086\n",
      "Epoch [25000/50000]  MSE: 0.000001  L1: 83.65  Loss: 0.000085\n",
      "Epoch [25500/50000]  MSE: 0.000002  L1: 83.87  Loss: 0.000086\n",
      "Epoch [26000/50000]  MSE: 0.000006  L1: 83.48  Loss: 0.000090\n",
      "Epoch [26500/50000]  MSE: 0.000004  L1: 83.10  Loss: 0.000087\n",
      "Epoch [27000/50000]  MSE: 0.000003  L1: 83.36  Loss: 0.000087\n",
      "Epoch [27500/50000]  MSE: 0.000005  L1: 83.89  Loss: 0.000089\n",
      "Epoch [28000/50000]  MSE: 0.000005  L1: 83.60  Loss: 0.000089\n",
      "Epoch [28500/50000]  MSE: 0.000003  L1: 82.69  Loss: 0.000085\n",
      "Epoch [29000/50000]  MSE: 0.000002  L1: 82.85  Loss: 0.000085\n",
      "Epoch [29500/50000]  MSE: 0.000002  L1: 83.17  Loss: 0.000085\n",
      "Epoch [30000/50000]  MSE: 0.000003  L1: 82.79  Loss: 0.000085\n",
      "Epoch [30500/50000]  MSE: 0.000002  L1: 82.92  Loss: 0.000085\n",
      "Epoch [31000/50000]  MSE: 0.000001  L1: 83.80  Loss: 0.000085\n",
      "Epoch [31500/50000]  MSE: 0.000002  L1: 82.66  Loss: 0.000084\n",
      "Epoch [32000/50000]  MSE: 0.000003  L1: 82.80  Loss: 0.000086\n",
      "Epoch [32500/50000]  MSE: 0.000001  L1: 82.55  Loss: 0.000083\n",
      "Epoch [33000/50000]  MSE: 0.000001  L1: 82.80  Loss: 0.000083\n",
      "Epoch [33500/50000]  MSE: 0.000055  L1: 82.67  Loss: 0.000138\n",
      "Epoch [34000/50000]  MSE: 0.000027  L1: 82.61  Loss: 0.000109\n",
      "Epoch [34500/50000]  MSE: 0.000001  L1: 82.27  Loss: 0.000083\n",
      "Epoch [35000/50000]  MSE: 0.000014  L1: 81.99  Loss: 0.000096\n",
      "Epoch [35500/50000]  MSE: 0.000012  L1: 82.19  Loss: 0.000094\n",
      "Epoch [36000/50000]  MSE: 0.000002  L1: 81.77  Loss: 0.000083\n",
      "Epoch [36500/50000]  MSE: 0.000001  L1: 82.11  Loss: 0.000083\n",
      "Epoch [37000/50000]  MSE: 0.000007  L1: 82.06  Loss: 0.000089\n",
      "Epoch [37500/50000]  MSE: 0.000001  L1: 81.84  Loss: 0.000083\n",
      "Epoch [38000/50000]  MSE: 0.000004  L1: 81.80  Loss: 0.000086\n",
      "Epoch [38500/50000]  MSE: 0.000102  L1: 82.16  Loss: 0.000184\n",
      "Epoch [39000/50000]  MSE: 0.000002  L1: 81.96  Loss: 0.000084\n",
      "Epoch [39500/50000]  MSE: 0.000002  L1: 82.93  Loss: 0.000085\n",
      "Epoch [40000/50000]  MSE: 0.000001  L1: 82.11  Loss: 0.000083\n",
      "Epoch [40500/50000]  MSE: 0.000004  L1: 81.89  Loss: 0.000085\n",
      "Epoch [41000/50000]  MSE: 0.000003  L1: 81.50  Loss: 0.000084\n",
      "Epoch [41500/50000]  MSE: 0.000014  L1: 81.25  Loss: 0.000096\n",
      "Epoch [42000/50000]  MSE: 0.000002  L1: 80.77  Loss: 0.000083\n",
      "Epoch [42500/50000]  MSE: 0.000006  L1: 80.65  Loss: 0.000087\n",
      "Epoch [43000/50000]  MSE: 0.000004  L1: 80.53  Loss: 0.000085\n",
      "Epoch [43500/50000]  MSE: 0.000001  L1: 80.46  Loss: 0.000082\n",
      "Epoch [44000/50000]  MSE: 0.000006  L1: 80.48  Loss: 0.000087\n",
      "Epoch [44500/50000]  MSE: 0.000006  L1: 80.54  Loss: 0.000087\n",
      "Epoch [45000/50000]  MSE: 0.000002  L1: 80.75  Loss: 0.000082\n",
      "Epoch [45500/50000]  MSE: 0.000006  L1: 80.50  Loss: 0.000086\n",
      "Epoch [46000/50000]  MSE: 0.000001  L1: 80.41  Loss: 0.000082\n",
      "Epoch [46500/50000]  MSE: 0.000002  L1: 80.44  Loss: 0.000082\n",
      "Epoch [47000/50000]  MSE: 0.000010  L1: 80.07  Loss: 0.000090\n",
      "Epoch [47500/50000]  MSE: 0.000001  L1: 80.51  Loss: 0.000081\n",
      "Epoch [48000/50000]  MSE: 0.000001  L1: 80.48  Loss: 0.000081\n",
      "Epoch [48500/50000]  MSE: 0.000004  L1: 79.81  Loss: 0.000084\n",
      "Epoch [49000/50000]  MSE: 0.000002  L1: 80.33  Loss: 0.000082\n",
      "Epoch [49500/50000]  MSE: 0.000002  L1: 80.41  Loss: 0.000083\n",
      "\n",
      "予測結果：\n",
      "[[ 0.9141924   0.11652982  0.13752736]\n",
      " [ 0.406892    0.742311    0.26674077]\n",
      " [ 0.2170136   0.32549268  0.63757455]\n",
      " [ 0.96055555  0.9214009   0.07772624]\n",
      " [ 0.4270357   0.7991357   0.8644967 ]\n",
      " [ 0.71678597  0.32057923  0.61601824]\n",
      " [-0.00437596  0.00132623 -0.00203407]\n",
      " [ 0.49861196  0.50080913  0.4979917 ]\n",
      " [ 0.99669826  1.0003643   0.9970095 ]\n",
      " [ 0.4499738   0.31103402  0.25359812]\n",
      " [ 0.76181185  0.56257427  0.49016923]\n",
      " [ 0.34855458  0.4764977   0.6091757 ]\n",
      " [ 0.3519328   0.42584854  0.24728209]\n",
      " [ 0.512474    0.5014645   0.68301797]\n",
      " [ 0.3697629   0.74295336  0.6747606 ]\n",
      " [ 0.86496025  0.47945237  0.18722406]\n",
      " [ 0.26589093  0.35597157  0.650278  ]\n",
      " [ 0.7713533   0.31326973  0.37128243]\n",
      " [ 0.3625384   0.23262927  0.4043324 ]\n",
      " [ 0.6100823   0.7378308   0.22784986]\n",
      " [ 0.88940233  0.6328876   0.15467134]\n",
      " [ 0.1458841   0.24648914  0.55808866]\n",
      " [ 0.22938725  0.58257455  0.27100813]\n",
      " [ 0.69412667  0.21423525  0.2194249 ]\n",
      " [ 0.9139243   0.7787162   0.10900649]\n",
      " [ 0.74895996  0.31094694  0.5678451 ]\n",
      " [-0.00416426  0.52121884  0.64406025]\n",
      " [ 0.943931    0.9478601   0.9162898 ]\n",
      " [ 0.7833645   0.7938215   0.7851963 ]\n",
      " [ 0.6253938   0.6350377   0.63092697]\n",
      " [ 0.47732812  0.47941     0.47387278]\n",
      " [ 0.33031425  0.33326572  0.33408204]\n",
      " [ 0.18729651  0.19384149  0.18740314]]\n",
      "\n",
      "C++ 用のパラメータファイル (model_parameters.h) を作成しました。\n"
     ]
    }
   ],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "# モデル定義\n",
    "class ColorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = ColorNet()\n",
    "\n",
    "# 損失関数（MSE）\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# L1正則化の係数 (重みを0を多くなるように定義している)\n",
    "lambda_l1 = 1e-6\n",
    "\n",
    "# 学習ループ\n",
    "epochs = 50000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_tensor)\n",
    "    mse = criterion(outputs, Y_tensor)\n",
    "\n",
    "    # --- L1 正則化-----------------------------\n",
    "    l1 = torch.tensor(0.0, requires_grad=False)\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            l1 = l1 + p.abs().sum()\n",
    "    loss = mse + lambda_l1 * l1\n",
    "    # -----------------------------------------\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch [{epoch}/{epochs}]  MSE: {mse.item():.6f}  L1: {l1.item():.2f}  Loss: {loss.item():.6f}')\n",
    "\n",
    "threshold = 0.01 # しきい値の設定\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_tensor)\n",
    "    print(\"\\n予測結果：\")\n",
    "    print(predictions.numpy())\n",
    "\n",
    "    \n",
    "# C++ 用の配列形式に変換する関数\n",
    "def convert_to_cpp_array(tensor: torch.Tensor, name: str, dtype: str = \"float\",threshold:float=0.01):\n",
    "    # Tensor → numpy 配列\n",
    "    flat = tensor.detach().numpy().flatten()\n",
    "    \n",
    "    # しきい値以下の値を0にする\n",
    "    # where(条件, 条件がTrueのときの値, 条件がFalseのときの値)\n",
    "    flat = np.where(np.abs(flat) < threshold, 0, flat)\n",
    "    \n",
    "    array_str = f\"{dtype} {name}[] = {{\"\n",
    "    array_str += \", \".join(map(str, flat))\n",
    "    array_str += \"};\"\n",
    "    return array_str\n",
    "\n",
    "\n",
    "\n",
    "# C++ 用のパラメータファイルを作成\n",
    "cpp_code = \"\"\n",
    "\n",
    "layer_idx = 1\n",
    "for layer in model.model:\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        cpp_code += convert_to_cpp_array(layer.weight, f\"weight_{layer_idx}\") + \"\\n\"\n",
    "        cpp_code += convert_to_cpp_array(layer.bias,   f\"bias_{layer_idx}\") + \"\\n\"\n",
    "        layer_idx += 1\n",
    "\n",
    "with open(\"model_parameters.h\", \"w\") as f:\n",
    "    f.write(cpp_code)\n",
    "\n",
    "print(\"\\nC++ 用のパラメータファイル (model_parameters.h) を作成しました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C++ 用のCSRパラメータファイル (model_parameters_csr.h) を作成しました。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# --- CSR形式への変換関数 ---\n",
    "def convert_to_csr_arrays(tensor: torch.Tensor, name: str, threshold: float = 0.01):\n",
    "    \"\"\"\n",
    "    PyTorchテンソルをCSR形式に変換し、C++用配列文字列として返す\n",
    "    \"\"\"\n",
    "    arr = tensor.detach().numpy()\n",
    "    arr = np.where(np.abs(arr) < threshold, 0, arr)\n",
    "\n",
    "    csr = csr_matrix(arr)\n",
    "\n",
    "    # CSRの主要3配列をC++配列として出力\n",
    "    data_str = f\"float {name}_data[] = {{ \" + \", \".join(map(str, csr.data)) + \" };\"\n",
    "    indices_str = f\"int {name}_indices[] = {{ \" + \", \".join(map(str, csr.indices)) + \" };\"\n",
    "    indptr_str = f\"int {name}_indptr[] = {{ \" + \", \".join(map(str, csr.indptr)) + \" };\"\n",
    "\n",
    "    shape_str = f\"int {name}_shape[2] = {{{csr.shape[0]}, {csr.shape[1]}}};\"\n",
    "\n",
    "    return \"\\n\".join([data_str, indices_str, indptr_str, shape_str])\n",
    "\n",
    "\n",
    "# --- Dense配列（biasなど）用 ---\n",
    "def convert_to_dense_array(tensor: torch.Tensor, name: str, dtype: str = \"float\", threshold: float = 0.01):\n",
    "    flat = tensor.detach().numpy().flatten()\n",
    "    flat = np.where(np.abs(flat) < threshold, 0, flat)\n",
    "    array_str = f\"{dtype} {name}[] = {{ \" + \", \".join(map(str, flat)) + \" };\"\n",
    "    return array_str\n",
    "\n",
    "\n",
    "# --- C++コード生成部分 ---\n",
    "cpp_code = \"\"\n",
    "\n",
    "layer_idx = 1\n",
    "for layer in model.model:\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        cpp_code += f\"// Layer {layer_idx}\\n\"\n",
    "        cpp_code += convert_to_csr_arrays(layer.weight, f\"weight_{layer_idx}\") + \"\\n\"\n",
    "        cpp_code += convert_to_dense_array(layer.bias, f\"bias_{layer_idx}\") + \"\\n\\n\"\n",
    "        layer_idx += 1\n",
    "\n",
    "with open(\"model_parameters_csr_1.h\", \"w\") as f:\n",
    "    f.write(cpp_code)\n",
    "\n",
    "print(\"\\nC++ 用のCSRパラメータファイル (model_parameters_csr.h) を作成しました。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
